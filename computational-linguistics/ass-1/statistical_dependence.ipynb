{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, the goal is the compute Pointwise mutual information (PMI) score for each token. It is measure of statistical dependence of the events  $ X_t = w_1$ and $X_{t+1} = w_2$. It is given by-\n",
    "$$\n",
    "pmi(w1 , w2) = log(\\frac{C(w1 w2)*N)}{(C(w1)*C(w2)})\n",
    "$$\n",
    "\n",
    "To begin with, we first tokenize the corpora as usual and strip the punctuations so as not to obtain a biased score that the punctuation tokens would otherwise introduce. As requested in the question, we also remove tokens with a net count of less than 10. Next, we also prepare the bigrams from the tokens obtained.\n",
    "Now the process is simple- we iterate through each of the bigrams and get the frequency their frequency as well as the frequency of both of the tokens that form the bigram. Once we have the counts we can compute the PMI value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# author : Sangeet Sagar\n",
    "# e-mail : sasa00001@stud.uni-saarland.de\n",
    "# Organization: UniversitÃ¤t des Saarlandes\n",
    "\n",
    "\"\"\"\n",
    "Calculate the pmi for all successive pairs (w1 , w2 ) of words in a corpus\n",
    "pmi(w1 , w2) = log[(C(w1 w2)*N) / (C(w1)*C(w2))]\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import operator\n",
    "import itertools\n",
    "import collections\n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def data_prep(filename):\n",
    "    \"\"\"Perform pre-processing steps in the input file and tokenize it.\n",
    "\n",
    "    Args:\n",
    "        filename (str): path to file\n",
    "\n",
    "    Returns:\n",
    "        list:tokens- list containing tokenized words of the input text file \n",
    "    \"\"\"\n",
    "\n",
    "    file_content = open(filename, 'r', encoding='utf-8-sig').read()\n",
    "    file_content = file_content.lower()\n",
    "    # Strip punctuations. Reference: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "    file_content = file_content.translate(\n",
    "        str.maketrans('', '', string.punctuation))\n",
    "    tokens_list = nltk.word_tokenize(file_content)\n",
    "    # Remove tokens with frequnecy less than 10\n",
    "    tokens_list = [item for item in tokens_list if collections.Counter(tokens_list)[\n",
    "        item] >= 10]\n",
    "\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "def compute_pmi(word_pair, N, global_dict):\n",
    "    \"\"\"Givem word-pairs and tokens from the corpora, compute the PMI score\n",
    "\n",
    "    Args:\n",
    "        word_pair (tuple): tuple of word pairs- (w1, w2)\n",
    "        N (list): length of corpors\n",
    "        global_dict (dict): dict consisting frequency of each tokens and token_pairs\n",
    "\n",
    "    Returns:\n",
    "        float: PMI score of the given word-pair\n",
    "    \"\"\"\n",
    "\n",
    "    counts = lookup(word_pair, global_dict)\n",
    "    return math.log(((counts[2] * N))/(counts[0] * counts[1]))\n",
    "\n",
    "\n",
    "def lookup(word_pair, global_dict):\n",
    "    \"\"\"Compute counts of each word in the word pair and the word pair itself in the list of all token pairs\n",
    "\n",
    "    Args:\n",
    "        word_pair (tuple): tuple of word pairs- (w1, w2)\n",
    "        global_dict (dict): dict consisting frequency of each tokens and token_pairs\n",
    "\n",
    "    Returns:\n",
    "        list: list containing counts of w1, w2 (in the tokens list) and counts of word_pair (in the token_pairs list)\n",
    "    \"\"\"\n",
    "\n",
    "    Cw1 = global_dict.get(word_pair[0])\n",
    "    Cw2 = global_dict.get(word_pair[1])\n",
    "\n",
    "    Cw1_w2 = global_dict.get(word_pair)\n",
    "    return [Cw1, Cw2, Cw1_w2]\n",
    "\n",
    "\n",
    "def print_scores(sort_tok_dict, l, rev=False):\n",
    "    \"\"\"Print PMI scores in a tabulated format\n",
    "\n",
    "    Args:\n",
    "        sort_tok_dict (dict): dictionary containing word-pairs are key and PMI scores as values\n",
    "        l (int): maximum word-pairs for which PMI scores have to be printed\n",
    "        rev (bool): Choice to reverse the dict. Defaults to False.\n",
    "    \"\"\"\n",
    "    # References: https://www.geeksforgeeks.org/python-get-first-n-keyvalue-pairs-in-given-dictionary/\n",
    "\n",
    "    if rev:\n",
    "        out = dict(itertools.islice(sort_tok_dict.items(),\n",
    "                                    len(sort_tok_dict)-l, len(sort_tok_dict)))\n",
    "        out = dict(sorted(out.items(), key=operator.itemgetter(1), reverse=False))\n",
    "    else:\n",
    "        out = dict(itertools.islice(sort_tok_dict.items(), l))\n",
    "    dash = '-' * 32\n",
    "    print(dash)\n",
    "    print('{:<10s}{:>10s}{:>12s}'.format(\"w1\", \"w2\", \"pmi\"))\n",
    "    print(dash)\n",
    "    for key, value in out.items():\n",
    "        print('{:<10s}{:>10s}{:>12s}'.format(\n",
    "            key[0], key[1], str(format(value, \".3f\"))))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"main function\"\"\"\n",
    "\n",
    "    filename = \"data/junglebook.txt\"\n",
    "    tokens = data_prep(filename)\n",
    "    N = len(tokens)\n",
    "    token_pairs = list(ngrams(tokens, 2))\n",
    "\n",
    "    # Get a dict with combined counts of unigrams and bigrams\n",
    "    global_dict = nltk.FreqDist(tokens + token_pairs)\n",
    "\n",
    "    # create a dict with keys= word pairs, and value= None\n",
    "    tok_dict = dict.fromkeys(token_pairs)\n",
    "    for word_pair, pmi_score in tok_dict.items():\n",
    "        pmi_score = compute_pmi(word_pair, N, global_dict)\n",
    "        tok_dict[word_pair] = pmi_score\n",
    "\n",
    "    sort_tok_dict = dict(\n",
    "        sorted(tok_dict.items(), key=operator.itemgetter(1), reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI scores are useful in a way that they help us interpret what words in the corpora carry the most context. Words with highest PMI scores have higher chances of occuring in pairs and thus these words carry more meaning. A good example can be `united states`, `fore paws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "w1                w2         pmi\n",
      "--------------------------------\n",
      "machua          appa       8.287\n",
      "literary     archive       8.130\n",
      "united        states       7.987\n",
      "darzees         wife       7.699\n",
      "archive   foundation       7.604\n",
      "cold           lairs       7.448\n",
      "gutenberg   literary       7.293\n",
      "stretched     myself       7.188\n",
      "petersen       sahib       7.131\n",
      "hind            legs       6.988\n",
      "fore            paws       6.910\n",
      "twenty          yoke       6.850\n",
      "whole           line       6.718\n",
      "electronic     works       6.706\n",
      "hind        flippers       6.687\n",
      "master         words       6.669\n",
      "years            ago       6.641\n",
      "bring           news       6.623\n",
      "mans             cub       6.606\n",
      "council         rock       6.505\n"
     ]
    }
   ],
   "source": [
    "print_scores(sort_tok_dict, l=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the words with the lowest PMI score can combine with any word and therefore do not carry meaning. These are generally pronouns and prepositions like `he`, `of`, `and` etc. They can combine with any words to complete the sentence grammatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "w1                w2         pmi\n",
      "--------------------------------\n",
      "he                of      -3.490\n",
      "his              the      -3.318\n",
      "the              not      -3.298\n",
      "little           the      -3.001\n",
      "the                a      -2.956\n",
      "the               be      -2.849\n",
      "a                his      -2.841\n",
      "said              of      -2.602\n",
      "he                he      -2.571\n",
      "the               no      -2.538\n",
      "in                in      -2.524\n",
      "and               is      -2.493\n",
      "a                the      -2.486\n",
      "the               if      -2.477\n",
      "they              of      -2.449\n",
      "of              they      -2.449\n",
      "very             the      -2.448\n",
      "do               the      -2.404\n",
      "to              they      -2.383\n",
      "the            could      -2.365\n"
     ]
    }
   ],
   "source": [
    "print_scores(sort_tok_dict, l=20, rev=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
